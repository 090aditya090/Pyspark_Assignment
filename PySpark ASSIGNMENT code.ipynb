{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d058ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pymongo\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #Creating spark session\n",
    "    spark = SparkSession.builder.appName(\"demo\").getOrCreate()\n",
    "\n",
    "    client = pymongo.MongoClient(\"mongodb://172.18.0.15:27017/neurolabDB\")\n",
    "    # database name\n",
    "    dataBase = client[\"pysparkDatabase\"]\n",
    "    # Collection  Name\n",
    "    collection_case = db[\"collection_case\"]\n",
    "    collection_region = db[\"collection_region\"]\n",
    "    collection_timeProvince = db[\"collection_timeProvince\"]\n",
    "\n",
    "    # Read the data from MongoDB collection and create a DataFrame\n",
    "    caseDf = spark.createDataFrame(list(collection_case.find()))\n",
    "    regionDf = spark.createDataFrame(list(collection_region.find()))\n",
    "    timeProvinceDf = spark.createDataFrame(list(collection_timeProvince.find()))\n",
    "\n",
    "    # Show the DataFrame\n",
    "    caseDf.show()\n",
    "    regionDf.show()\n",
    "    timeProvinceDf.show()\n",
    "    \n",
    "    # create temp view\n",
    "    caseDf.createOrReplaceTempView(\"case\")\n",
    "    regionDf.createOrReplaceTempView(\"region\")\n",
    "    timeProvinceDf.createOrReplaceTempView(\"timeProvinve\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a29e133",
   "metadata": {},
   "source": [
    "a. Read the data, show it and Count the number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e0cd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from case\").show()\n",
    "spark.sql(\"select count(*) as count_rercords from case\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1279de3",
   "metadata": {},
   "source": [
    "b. Describe the data with a describe function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603a89f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "caseDf.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfc3a7f",
   "metadata": {},
   "source": [
    "c. If there is any duplicate value drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70ca5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df = timeProvinveDf.dropDuplicates(subset='date')\n",
    "Df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea2cc81",
   "metadata": {},
   "source": [
    "d. Use limit function for showcasing a limited number of\n",
    "records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dcc09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "regionDf.limit(2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609d09df",
   "metadata": {},
   "source": [
    "e. If you find the column name is not suitable, change the\n",
    "column name.[optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c136dd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RegionDf.withColumnRenamed(\"nursing_home_count\", \"nursingHomeCount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9e6cd7",
   "metadata": {},
   "source": [
    "f. Select the subset of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In PySpark, you can select a subset of columns from a DataFrame by passing a list of column names to the select method. Here is an example:\n",
    "columns_to_keep = [\"date\", \"time\", \"province\"]\n",
    "df = timeProvinceDf.select([col(c) for c in columns_to_keep])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfcb33e",
   "metadata": {},
   "source": [
    "g. If there is any null value, fill it with any random value or drop\n",
    "it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0d90f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the null values in a column using the fillna method\n",
    "df = caseDf.fillna(0, subset=[\"longitude\"])\n",
    "\n",
    "#drop the rows with null values using the dropna method\n",
    "df = caseDf.dropna(subset=[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1431b87e",
   "metadata": {},
   "source": [
    "h. Filter the data based on different columns or variables and\n",
    "do the best analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ad682",
   "metadata": {},
   "outputs": [],
   "source": [
    "caseDf.filter(caseDf[\"group\"] = \"True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728b5203",
   "metadata": {},
   "source": [
    "i. Sort the number of confirmed cases. Confirmed column is\n",
    "there in the dataset. Check with descending sort also.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab77ad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "caseDf.sort(ascending=False, \"Confirmed\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3568ee",
   "metadata": {},
   "source": [
    "j. In case of any wrong data type, cast that data type from\n",
    "integer to string or string to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431d468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have a column named \"age\" of data type integer and you want to change it to string\n",
    "df = df.withColumn(\"age\", df[\"age\"].cast(StringType()))\n",
    "\n",
    "# if you want to change a column from string to integer\n",
    "df = df.withColumn(\"age\", df[\"age\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a919990",
   "metadata": {},
   "source": [
    "k. Use group by on top of province and city column and agg it\n",
    "with sum of confirmed cases. For example\n",
    "df.groupBy([\"province\",\"city\"]).agg(function.sum(\"co\n",
    "nfirmed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b273189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = caseDf.groupBy([\"province\",\"city\"]).agg(sum(\"confirmed\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6f93c8",
   "metadata": {},
   "source": [
    "l. For joins we will need one more file.you can use region file.\n",
    "User different different join methods.for example\n",
    "cases.join(regions, ['province','city'],how='left')\n",
    "You can do your best analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bb311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "caseDf.join(regionDf, ['province','city'],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de7b842",
   "metadata": {},
   "source": [
    "5. If you want, you can also use SQL with data frames. Let us try to\n",
    "run some SQL on the cases table.\n",
    "For example:\n",
    "cases.registerTempTable('cases_table')\n",
    "newDF = sqlContext.sql('select * from cases_table where\n",
    "confirmed>100')\n",
    "newDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420c5d33",
   "metadata": {},
   "source": [
    "Here is a example how you can use df for sql now you can perform\n",
    "various operations with GROUP BY, HAVING, AND ORDER BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399fa40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "caseDf.createOrReplaceTempView(\"case\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM case GROUP BY province HAVING group = 'true' ORDER BY confirmed\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a0e287",
   "metadata": {},
   "source": [
    "6. Create Spark UDFs\n",
    "Create function casehighlow()\n",
    "If case is less than 50 return low else return high\n",
    "convert into a UDF Function and mention the return type of\n",
    "function.\n",
    "Note: You can create as many as udf based on analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433f3ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def casehighlow(confirmed_cases):\n",
    "    if confirmed_cases < 50:\n",
    "        return \"low\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "casehighlow_udf = udf(casehighlow, StringType())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
